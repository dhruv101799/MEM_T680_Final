{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Import required libraries\n",
    "\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import urllib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a tool I have provided you to help you download your file.\n",
    "\n",
    "def download_file(url, filename):\n",
    "    \"\"\"\n",
    "    A function that downloads the data file from a URL\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : string\n",
    "        url where the file to download is located\n",
    "    filename : string\n",
    "        location where to save the file\n",
    "    reporthook : function\n",
    "        callback to display the download progress\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(filename):\n",
    "        urllib.request.urlretrieve(url, filename, reporthook)\n",
    "        \n",
    "def reporthook(count, block_size, total_size):\n",
    "    \"\"\"\n",
    "    A function that displays the status and speed of the download\n",
    "    \"\"\"\n",
    "\n",
    "    global start_time\n",
    "    if count == 0:\n",
    "        start_time = time.time()\n",
    "        return\n",
    "    duration = time.time() - start_time\n",
    "    progress_size = int(count * block_size)\n",
    "    speed = int(progress_size / (1024 * duration + 0.0001))\n",
    "    percent = int(count * block_size * 100 / total_size)\n",
    "    sys.stdout.write(\"\\r...%d%%, %d MB, %d KB/s, %d seconds passed\" %\n",
    "                     (percent, progress_size / (1024 * 1024), speed, duration))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can download your file by typing your first name into the name block\n",
    "# The name used is the first part of your first name as listed in BB learn\n",
    "# If you have problems downloading the data please reach out to me\n",
    "\n",
    "name = 'Dhruv'\n",
    "# download_file(f'https://zenodo.org/record/7339649/files/data_{name}.npz?download=1','data.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_feat: (100000, 30)\n",
      "training_true: (100000, 3)\n",
      "validation_feat: (65536, 30)\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "\n",
    "data = np.load('data.npz')\n",
    "data_list = data.files\n",
    "for item in data_list:\n",
    "    print(f\"{item}: {data[item].shape}\")\n",
    "\n",
    "# print(data[\"training_true\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100000, 30])\n",
      "torch.Size([100000, 3])\n"
     ]
    }
   ],
   "source": [
    "## Create tensors from numpy arrays\n",
    "\n",
    "# Training features\n",
    "training_feat_np = data[data_list[0]]\n",
    "scaler = StandardScaler()\n",
    "training_feat_scaled = scaler.fit(training_feat_np)\n",
    "training_feat_scaled = scaler.transform(training_feat_np)\n",
    "X = torch.from_numpy(training_feat_scaled)\n",
    "print(X.shape)\n",
    "# Target Values\n",
    "training_true_np = data[data_list[1]]\n",
    "y = torch.from_numpy(training_true_np)\n",
    "print(y.shape)\n",
    "# Validation Features\n",
    "validation_feat_np = data[data_list[2]]\n",
    "X_test = torch.from_numpy(validation_feat_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range for results: -1.740495746836084 1.7395879139777914\n"
     ]
    }
   ],
   "source": [
    "print(\"Range for results:\", np.min(training_feat_scaled), np.max(training_feat_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "  '''Dataset Class to store the samples and their corresponding labels, \n",
    "  and DataLoader wraps an iterable around the Dataset to enable easy access to the samples.\n",
    "  '''\n",
    "\n",
    "  def __init__(self, X: np.ndarray, y: np.ndarray, device = 'cpu') -> None:\n",
    "\n",
    "    # need to convert float64 to float32 else \n",
    "    # will get the following error\n",
    "    # RuntimeError: expected scalar type Double but found Float\n",
    "    self.X = torch.from_numpy(X.astype(np.float32)).to(device)\n",
    "    self.y = torch.from_numpy(y.astype(np.float32)).to(device)\n",
    "    self.len = self.X.shape[0]\n",
    "  \n",
    "  def __getitem__(self, index: int) -> tuple:\n",
    "    return self.X[index], self.y[index]\n",
    "\n",
    "  def __len__(self) -> int:\n",
    "    return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into test/train\n",
    "split_dataset = random_split(training_feat_np, \n",
    "                             lengths=[2/3, 1/3], \n",
    "                             generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "train = np.array(split_dataset[0][:])\n",
    "test = np.array(split_dataset[1])\n",
    "\n",
    "# Configure dataset\n",
    "# training_data = Data(X=training_feat_np, y=training_true_np)\n",
    "# testing_data = Data(X=validation_feat_np, y=training_true_np)\n",
    "\n",
    "training_data = Data(X=train, y=training_true_np)\n",
    "testing_data = Data(X=test, y=training_true_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 30])\n",
      "Shape of y: torch.Size([64, 3]) torch.float32\n",
      "Shape of X [N, C, H, W]: torch.Size([64, 30])\n",
      "Shape of y: torch.Size([64, 3]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(testing_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for X, y in train_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of features (len of X cols)\n",
    "input_dim =train.shape[1]\n",
    "# number of hidden layers set this to 50\n",
    "hidden_layers = 50\n",
    "# Add the number of output dimensions\n",
    "output_dim = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network(nn.Module):\n",
    "  ''' Regression Model\n",
    "  ''' \n",
    "\n",
    "  # note, you can ignore the `:int` and `-> None` this is just more advanced doctring syntax\n",
    "  def __init__(self, input_dim: int, hidden_dim: int, output_dim: int) -> None:\n",
    "      '''The network has 4 layers\n",
    "            - input layer\n",
    "            - ReLu\n",
    "            - hidden layer\n",
    "            - ReLu\n",
    "            - hidden layer\n",
    "            - ReLu\n",
    "            - output layer\n",
    "      '''\n",
    "      super(Neural_Network, self).__init__()\n",
    "      # in this part you should intantiate each of the layer components\n",
    "      # Type your code here\n",
    "      self.flatten = nn.Flatten()\n",
    "      self.linear_relu_stack = nn.Sequential(\n",
    "        nn.Linear(input_dim, hidden_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_dim, hidden_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_dim, hidden_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_dim, output_dim),\n",
    "      )\n",
    "\n",
    "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "      # In this part you should build a model that returns the 3 outputs of the regression\n",
    "      # Type your code here\n",
    "      x = self.linear_relu_stack(x)\n",
    "      \n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural_Network(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=30, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=50, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# initiate the regression model\n",
    "# make sure to put it on your GPU\n",
    "model = Neural_Network(input_dim, hidden_layers, output_dim)\n",
    "model = model.cpu()\n",
    "print(model)\n",
    "\n",
    "# criterion to computes the loss between input and target\n",
    "# Choose a good criteria\n",
    "\n",
    "# optimizer that will be used to update weights and biases\n",
    "# you can choose any optimizer. I would recommend ADAM.\n",
    "# This problem should not be hard to optimize. A good starting learning rate is 3e-5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 3e-5\n",
    "batch_size = 64\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    \n",
    "    pred_vals = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            pred_vals.append(pred.detach().to('cpu').numpy())\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(\n",
    "        f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\"\n",
    "    )\n",
    "    return np.concatenate(pred_vals, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.897649  [    0/66667]\n",
      "loss: 0.147759  [ 6400/66667]\n",
      "loss: 0.093400  [12800/66667]\n",
      "loss: 0.077079  [19200/66667]\n",
      "loss: 0.061563  [25600/66667]\n",
      "loss: 0.047413  [32000/66667]\n",
      "loss: 0.052885  [38400/66667]\n",
      "loss: 0.048684  [44800/66667]\n",
      "loss: 0.040755  [51200/66667]\n",
      "loss: 0.038482  [57600/66667]\n",
      "loss: 0.042539  [64000/66667]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.035761  [    0/66667]\n",
      "loss: 0.037223  [ 6400/66667]\n",
      "loss: 0.040420  [12800/66667]\n",
      "loss: 0.036941  [19200/66667]\n",
      "loss: 0.031781  [25600/66667]\n",
      "loss: 0.037191  [32000/66667]\n",
      "loss: 0.042082  [38400/66667]\n",
      "loss: 0.032855  [44800/66667]\n",
      "loss: 0.029056  [51200/66667]\n",
      "loss: 0.034608  [57600/66667]\n",
      "loss: 0.028082  [64000/66667]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.030303  [    0/66667]\n",
      "loss: 0.030936  [ 6400/66667]\n",
      "loss: 0.029152  [12800/66667]\n",
      "loss: 0.035777  [19200/66667]\n",
      "loss: 0.028824  [25600/66667]\n",
      "loss: 0.028411  [32000/66667]\n",
      "loss: 0.032123  [38400/66667]\n",
      "loss: 0.030005  [44800/66667]\n",
      "loss: 0.030608  [51200/66667]\n",
      "loss: 0.030804  [57600/66667]\n",
      "loss: 0.038483  [64000/66667]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.027682  [    0/66667]\n",
      "loss: 0.029090  [ 6400/66667]\n",
      "loss: 0.027143  [12800/66667]\n",
      "loss: 0.025227  [19200/66667]\n",
      "loss: 0.031858  [25600/66667]\n",
      "loss: 0.032579  [32000/66667]\n",
      "loss: 0.026929  [38400/66667]\n",
      "loss: 0.030810  [44800/66667]\n",
      "loss: 0.028027  [51200/66667]\n",
      "loss: 0.030788  [57600/66667]\n",
      "loss: 0.028280  [64000/66667]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.031760  [    0/66667]\n",
      "loss: 0.025557  [ 6400/66667]\n",
      "loss: 0.029984  [12800/66667]\n",
      "loss: 0.023919  [19200/66667]\n",
      "loss: 0.027500  [25600/66667]\n",
      "loss: 0.019556  [32000/66667]\n",
      "loss: 0.022468  [38400/66667]\n",
      "loss: 0.026735  [44800/66667]\n",
      "loss: 0.026476  [51200/66667]\n",
      "loss: 0.028606  [57600/66667]\n",
      "loss: 0.029546  [64000/66667]\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.024413  [    0/66667]\n",
      "loss: 0.026713  [ 6400/66667]\n",
      "loss: 0.023758  [12800/66667]\n",
      "loss: 0.026597  [19200/66667]\n",
      "loss: 0.030685  [25600/66667]\n",
      "loss: 0.025876  [32000/66667]\n",
      "loss: 0.029685  [38400/66667]\n",
      "loss: 0.023757  [44800/66667]\n",
      "loss: 0.022729  [51200/66667]\n",
      "loss: 0.026436  [57600/66667]\n",
      "loss: 0.022675  [64000/66667]\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.024480  [    0/66667]\n",
      "loss: 0.028346  [ 6400/66667]\n",
      "loss: 0.024988  [12800/66667]\n",
      "loss: 0.027246  [19200/66667]\n",
      "loss: 0.029970  [25600/66667]\n",
      "loss: 0.024978  [32000/66667]\n",
      "loss: 0.029514  [38400/66667]\n",
      "loss: 0.029043  [44800/66667]\n",
      "loss: 0.024698  [51200/66667]\n",
      "loss: 0.024371  [57600/66667]\n",
      "loss: 0.023242  [64000/66667]\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.024193  [    0/66667]\n",
      "loss: 0.023069  [ 6400/66667]\n",
      "loss: 0.022735  [12800/66667]\n",
      "loss: 0.027224  [19200/66667]\n",
      "loss: 0.025541  [25600/66667]\n",
      "loss: 0.026273  [32000/66667]\n",
      "loss: 0.025900  [38400/66667]\n",
      "loss: 0.031080  [44800/66667]\n",
      "loss: 0.025673  [51200/66667]\n",
      "loss: 0.024275  [57600/66667]\n",
      "loss: 0.026358  [64000/66667]\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.023581  [    0/66667]\n",
      "loss: 0.027051  [ 6400/66667]\n",
      "loss: 0.024599  [12800/66667]\n",
      "loss: 0.029511  [19200/66667]\n",
      "loss: 0.028561  [25600/66667]\n",
      "loss: 0.026202  [32000/66667]\n",
      "loss: 0.023043  [38400/66667]\n",
      "loss: 0.023201  [44800/66667]\n",
      "loss: 0.026860  [51200/66667]\n",
      "loss: 0.026841  [57600/66667]\n",
      "loss: 0.023918  [64000/66667]\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.024201  [    0/66667]\n",
      "loss: 0.025814  [ 6400/66667]\n",
      "loss: 0.027217  [12800/66667]\n",
      "loss: 0.026852  [19200/66667]\n",
      "loss: 0.027676  [25600/66667]\n",
      "loss: 0.027679  [32000/66667]\n",
      "loss: 0.029000  [38400/66667]\n",
      "loss: 0.027110  [44800/66667]\n",
      "loss: 0.026873  [51200/66667]\n",
      "loss: 0.027468  [57600/66667]\n",
      "loss: 0.024084  [64000/66667]\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.029334  [    0/66667]\n",
      "loss: 0.025604  [ 6400/66667]\n",
      "loss: 0.022262  [12800/66667]\n",
      "loss: 0.026853  [19200/66667]\n",
      "loss: 0.025424  [25600/66667]\n",
      "loss: 0.029271  [32000/66667]\n",
      "loss: 0.026202  [38400/66667]\n",
      "loss: 0.023262  [44800/66667]\n",
      "loss: 0.025523  [51200/66667]\n",
      "loss: 0.024870  [57600/66667]\n",
      "loss: 0.026795  [64000/66667]\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.026631  [    0/66667]\n",
      "loss: 0.024033  [ 6400/66667]\n",
      "loss: 0.023590  [12800/66667]\n",
      "loss: 0.026107  [19200/66667]\n",
      "loss: 0.023540  [25600/66667]\n",
      "loss: 0.026431  [32000/66667]\n",
      "loss: 0.023750  [38400/66667]\n",
      "loss: 0.023187  [44800/66667]\n",
      "loss: 0.027155  [51200/66667]\n",
      "loss: 0.025668  [57600/66667]\n",
      "loss: 0.025757  [64000/66667]\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.023794  [    0/66667]\n",
      "loss: 0.026251  [ 6400/66667]\n",
      "loss: 0.022262  [12800/66667]\n",
      "loss: 0.026317  [19200/66667]\n",
      "loss: 0.021339  [25600/66667]\n",
      "loss: 0.025929  [32000/66667]\n",
      "loss: 0.025822  [38400/66667]\n",
      "loss: 0.027107  [44800/66667]\n",
      "loss: 0.021085  [51200/66667]\n",
      "loss: 0.024971  [57600/66667]\n",
      "loss: 0.025964  [64000/66667]\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.024058  [    0/66667]\n",
      "loss: 0.025567  [ 6400/66667]\n",
      "loss: 0.026330  [12800/66667]\n",
      "loss: 0.025447  [19200/66667]\n",
      "loss: 0.024563  [25600/66667]\n",
      "loss: 0.022074  [32000/66667]\n",
      "loss: 0.027598  [38400/66667]\n",
      "loss: 0.028510  [44800/66667]\n",
      "loss: 0.023053  [51200/66667]\n",
      "loss: 0.024882  [57600/66667]\n",
      "loss: 0.024830  [64000/66667]\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.027053  [    0/66667]\n",
      "loss: 0.023895  [ 6400/66667]\n",
      "loss: 0.025976  [12800/66667]\n",
      "loss: 0.025679  [19200/66667]\n",
      "loss: 0.021925  [25600/66667]\n",
      "loss: 0.025468  [32000/66667]\n",
      "loss: 0.026227  [38400/66667]\n",
      "loss: 0.021880  [44800/66667]\n",
      "loss: 0.025265  [51200/66667]\n",
      "loss: 0.027895  [57600/66667]\n",
      "loss: 0.025396  [64000/66667]\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.023129  [    0/66667]\n",
      "loss: 0.023831  [ 6400/66667]\n",
      "loss: 0.020438  [12800/66667]\n",
      "loss: 0.025037  [19200/66667]\n",
      "loss: 0.024104  [25600/66667]\n",
      "loss: 0.021085  [32000/66667]\n",
      "loss: 0.026389  [38400/66667]\n",
      "loss: 0.025478  [44800/66667]\n",
      "loss: 0.025562  [51200/66667]\n",
      "loss: 0.024008  [57600/66667]\n",
      "loss: 0.023719  [64000/66667]\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.025605  [    0/66667]\n",
      "loss: 0.024257  [ 6400/66667]\n",
      "loss: 0.025148  [12800/66667]\n",
      "loss: 0.026102  [19200/66667]\n",
      "loss: 0.028000  [25600/66667]\n",
      "loss: 0.021376  [32000/66667]\n",
      "loss: 0.023237  [38400/66667]\n",
      "loss: 0.024499  [44800/66667]\n",
      "loss: 0.025432  [51200/66667]\n",
      "loss: 0.023908  [57600/66667]\n",
      "loss: 0.025920  [64000/66667]\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.029261  [    0/66667]\n",
      "loss: 0.030527  [ 6400/66667]\n",
      "loss: 0.026452  [12800/66667]\n",
      "loss: 0.028866  [19200/66667]\n",
      "loss: 0.024635  [25600/66667]\n",
      "loss: 0.024351  [32000/66667]\n",
      "loss: 0.022838  [38400/66667]\n",
      "loss: 0.025626  [44800/66667]\n",
      "loss: 0.022687  [51200/66667]\n",
      "loss: 0.023220  [57600/66667]\n",
      "loss: 0.026861  [64000/66667]\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.031076  [    0/66667]\n",
      "loss: 0.021626  [ 6400/66667]\n",
      "loss: 0.023635  [12800/66667]\n",
      "loss: 0.025458  [19200/66667]\n",
      "loss: 0.023432  [25600/66667]\n",
      "loss: 0.024135  [32000/66667]\n",
      "loss: 0.025288  [38400/66667]\n",
      "loss: 0.023817  [44800/66667]\n",
      "loss: 0.021074  [51200/66667]\n",
      "loss: 0.025579  [57600/66667]\n",
      "loss: 0.020926  [64000/66667]\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.023084  [    0/66667]\n",
      "loss: 0.020836  [ 6400/66667]\n",
      "loss: 0.022098  [12800/66667]\n",
      "loss: 0.023828  [19200/66667]\n",
      "loss: 0.022624  [25600/66667]\n",
      "loss: 0.021193  [32000/66667]\n",
      "loss: 0.023688  [38400/66667]\n",
      "loss: 0.026220  [44800/66667]\n",
      "loss: 0.026425  [51200/66667]\n",
      "loss: 0.020501  [57600/66667]\n",
      "loss: 0.026306  [64000/66667]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (64) must match the size of tensor b (3) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [177], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m      2\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m-------------------------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m     test_loop(test_dataloader, model, loss_fn)\n\u001b[0;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDone!\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn [172], line 29\u001b[0m, in \u001b[0;36mtest_loop\u001b[1;34m(dataloader, model, loss_fn)\u001b[0m\n\u001b[0;32m     27\u001b[0m         pred_vals\u001b[39m.\u001b[39mappend(pred\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mnumpy())\n\u001b[0;32m     28\u001b[0m         test_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss_fn(pred, y)\u001b[39m.\u001b[39mitem()\n\u001b[1;32m---> 29\u001b[0m         correct \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (pred\u001b[39m.\u001b[39;49margmax(\u001b[39m1\u001b[39;49m) \u001b[39m==\u001b[39;49m y)\u001b[39m.\u001b[39mtype(torch\u001b[39m.\u001b[39mfloat)\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     31\u001b[0m test_loss \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m num_batches\n\u001b[0;32m     32\u001b[0m correct \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m size\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (3) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('ML_T680')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "80c4cda4b32ad6e16c64aa1fd6e254f2a6a75936c807cf5e21680d140147b0d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
